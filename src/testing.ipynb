{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.data.utils as ttdutils\n",
    "\n",
    "\n",
    "from text_dataset import TextDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10053"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = WikiText2(root=\"../data\", split=\"train\")\n",
    "# slice to 30 lines\n",
    "train_iter = list(train_iter)[:2000]\n",
    "tokenizer = ttdutils.get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([117249])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [torch.LongTensor([vocab(tokenizer(item))]) for item in train_iter]\n",
    "data = tuple(filter(lambda x: x.numel() > 0, data))\n",
    "data = torch.cat(data, dim=1).squeeze(0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return torch.stack(batch, dim=1)\n",
    "\n",
    "dataset = TextDataset(data, seq_len=32)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "['land', 'super', 'mario', 'land', '3', 'began', 'the', 'wario', 'franchise', '.', 'after', '19', 'years', ',', 'the', '2011', 'title', 'super', 'mario', '3d', 'land', 'for', 'the', 'nintendo', '3ds', 'became', 'mario', \"'\", 's', 'first', 'game', 'in']\n",
      "torch.Size([32])\n",
      "['the', 'species', '.', '=', '=', 'conservation', '=', '=', '<unk>', 'records', 'indicate', 'that', 'in', 'pre', '@-@', 'polynesian', 'times', ',', 'the', 'kakapo', 'was', 'new', 'zealand', \"'\", 's', 'third', 'most', 'common', 'bird', 'and', 'it', 'was']\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        for xpart in x.T:\n",
    "            print(xpart.shape)\n",
    "            print(vocab.lookup_tokens(xpart.tolist()))\n",
    "        # print(vocab.lookup_tokens(x.squeeze(1).tolist()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 768\n",
    "emb = nn.Embedding(len(vocab), embed_dim)\n",
    "model = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embed_dim, nhead=12, dim_feedforward=3072, dropout=0.1, activation='gelu'), num_layers=6)\n",
    "decoder = nn.Linear(embed_dim, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = emb(x)\n",
    "out.shape\n",
    "\n",
    "eps = model(out)\n",
    "eps.shape\n",
    "\n",
    "decoded = decoder(eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([32, 1, 28782])\n",
      "x shape: torch.Size([32, 1])\n",
      "tensor(10.3937, grad_fn=<NllLoss2DBackward0>)\n",
      "\n",
      "\n",
      "Indices to text:\n",
      "torch.Size([32, 1])\n",
      "sculpture infants gallian corpse elinor swaziland disbanded birmingham meng potatoes preventing emily rotational squirrels isabella watershed thickly birmingham situ clues bout bulldogs receivership isabella 864 presumed desmond microlight infants gallian corpse sensitive\n"
     ]
    }
   ],
   "source": [
    "logits = F.log_softmax(decoded, dim=-1)\n",
    "logits_permuted = logits.permute(0, 2, 1)\n",
    "indices = torch.argmax(logits, dim=-1)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"x shape:\", x.shape)\n",
    "\n",
    "\n",
    "\n",
    "loss = F.cross_entropy(logits_permuted, x)\n",
    "print(loss)\n",
    "\n",
    "print(\"\\n\\nIndices to text:\")\n",
    "print(indices.shape)\n",
    "tokens = [vocab.lookup_tokens(i.tolist()) for i in indices.T]\n",
    "# print(tokens)\n",
    "sentences = [\" \".join(token) for token in tokens]\n",
    "print(sentences[0])\n",
    "# vocab.lookup_tokens(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58b3ebf8dec169d8aec70c36b552225c97668c5e5a4c1d2a670fb746efb28189"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
